---
title: "Auto-Gaze Gradient Boosting Machine"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T, warning = F, message = F,
                      fig.width = 6, fig.asp = 0.63)
```

```{r}
library(cowplot); library(magrittr); library(DMwR); library(stringr);
library(tidyverse); library(caret); library(here); 
set.seed(96)
data_path <- "data/soc_ref/ground_truth/02_tidy"
features_path <- "data/soc_ref/open_face_output"
theme_set(ggthemes::theme_few())
```

```{r}
d <- read_csv(here::here(data_path, "ground_truth_processed_tidy.csv"))
```

## Preprocess the data

Select only the features that relevant for training the classifer

```{r}
d_model <- d %>% 
  filter(success == 1, !is.na(gold_code)) %>% 
  select(gaze_0_x:gold_code) %>% 
  mutate(gold_code = as.factor(gold_code)) 
```

How many looks vs. no looks in the gold set?

```{r}
table(d_model$gold_code) 
```

## Visualize predictors

```{r}
d_model %>% 
  GGally::ggpairs(data = ., aes(color = gold_code, alpha = 0.6),
          columns = c("gaze_0_x", "gaze_0_y", "gaze_angle_x", "gaze_angle_y", 
                      "pose_Rx", "pose_Ry", "pose_Ty"))
```

Looks like gaze angle X and Y are perfectly correlated with gaze_X and gaze_Y. Generally, we want to remove attributes with an absolute correlation of 0.75 or higher.

```{r}
d_model %<>% select(-gaze_angle_x, -gaze_angle_y)
```

## Split the data

Here we are using a 70% training and and 30% test split.

```{r}
trainIndex <- createDataPartition(d_model$gold_code, p = .7, 
                                  list = FALSE, 
                                  times = 1)
head(trainIndex)
```

```{r}
train_data <- d_model[trainIndex, ]
test_data <- d_model[-trainIndex, ]
```

Preprocess (center and scale) the predictor variables.

```{r}
preProcValues <- preProcess(select(d_model, -gold_code), method = c("center", "scale", "nzv"))
trainTransformed <- predict(preProcValues, train_data)
testTransformed <- predict(preProcValues, test_data)
```

See how many instances of each class we have in the training and test data.

```{r}
table(trainTransformed$gold_code) # training
table(testTransformed$gold_code) # test
```

## Fit model

10-fold cross validation. K-fold cross validation is a method for estimating a tuning parameter λ (such as subset size): Divides the data into K roughly equal parts; for each k = 1, 2, . . . K, fit the model with parameter λ to the other K − 1 parts, and compute its error in predicting the kth part; do this for many values of λ and choose the value of λ that makes CV error smallest. K = 5 or 10 is typical.

```{r}
fitControl <- trainControl(method = "repeatedcv", #k-fold cross validation
                           number = 10, #k (number of folds) = 10
                           repeats = 10, # 10 separate k-fold cross-validations are performed
                           sampling = "smote", #synthetic minority oversampling technique; over-sample sparse class
                           savePredictions = T,
                           classProbs = T,
                           summaryFunction = twoClassSummary)
```

## Model tuning

### Fit decision tree with stochastic gradient boosting. 

The hyperparameters include:

* n.trees (# Boosting Iterations)
* interaction.depth (Max Tree Depth)
* shrinkage (Shrinkage)
* n.minobsinnode (Min. Terminal Node Size)

[todo: say more about what the GBM is and what the different hyperparameters control]

```{r}
gbmGrid <-  expand.grid(interaction.depth = c(1, 5, 9), 
                        n.trees = c(100, 200, 300), 
                        shrinkage = 0.1,
                        n.minobsinnode = 20)

gbmFit <- train(gold_code ~ ., 
                 data = trainTransformed, 
                 method = "gbm", 
                 trControl = fitControl, 
                 verbose = FALSE, 
                 tuneGrid = gbmGrid)
gbmFit
```

KM notes on GBM hyperparameters:
  * fitting with interaction.depth of 1 doesn't do well, so I dropped this option
  * you start to see diminishing returns on Kappa and Accuracy after 300 trees and interaction.depth 20

Save model for future use.

```{r}
write_rds(gbmFit, here::here("R/classifiers/gbm_model.rds"))
```

## Evaluate model performance

### Plot accuracy and kappa

The Kappa Statistic compares the accuracy of the system to the accuracy of a random system. There is not a standardized interpretation of the kappa statistic. According to Wikipedia (citing their paper), Landis and Koch considers 0-0.20 as slight, 0.21-0.40 as fair, 0.41-0.60 as moderate, 0.61-0.80 as substantial, and 0.81-1 as almost perfect.

### Plot ROC

```{r}
ggplot(gbmFit, metric = "ROC") + labs(title="ROC")
```

### Variable importance

[todo: say more about what variable importance means]

```{r}
library(gbm)
gbmImp <- varImp(gbmFit, scale = FALSE)
plot(gbmImp)
```

Looks like pose_TZ and pose_TY are much more important than the rest of the predictors. Let's visualize the relationship between these predictors, gaze vectors, and gold codes.

```{r}
d_model %>% 
  GGally::ggpairs(data = ., aes(color = gold_code,  alpha = 0.6),
          columns = c("gaze_0_x", "gaze_0_y", "pose_Rx", "pose_Ty", "pose_Tz"))
```

### Accuracy

Get predictions from model on held out test set.

```{r get preds}
testTransformed$pred <- predict(gbmFit, newdata = testTransformed)
testTransformed %<>% mutate(correct_pred = ifelse(gold_code == pred, 1, 0))
```

Compute some stats to see how well did. First, accuracy overall:

```{r}
testTransformed %>% pull(correct_pred) %>% mean() %>% round(3)
```

Accuracy within each class:

```{r}
testTransformed %>% 
  group_by(gold_code) %>% 
  summarise(n = n(),
            acc = mean(correct_pred) %>% round(3))
```

### Precision and recall

* Precision (also called positive predictive value) is the fraction of relevant instances among the retrieved instances. 

* Recall (also known as sensitivity) is the fraction of relevant instances that have been retrieved over the total amount of relevant instances

```{r}
testTransformed %<>% 
  mutate(output_type = case_when(
    pred == "look" & gold_code == "look" ~ "true_positive",
    pred == "look" & gold_code == "no_look" ~ "false_positive",
    pred == "no_look" & gold_code == "no_look" ~ "true_negative",
    pred == "no_look" & gold_code == "look" ~ "false_negative")
    )

results <- testTransformed %>% count(output_type) 
```

Now, compute model performance.

```{r}
source(here::here("R/helpers/model_evals.R"))

precision <- compute_precision(true_positives = results$n[4], false_positives = results$n[2])
paste0("precision is: ", precision)

recall <- compute_recall(true_positives = results$n[4], false_negatives = results$n[1])
paste0("recall is: ", recall)

f_score <- compute_f(precision, recall)
paste0("f-score is: ", f_score)
```

## Predict on the full dataset

```{r}
d_features_preds <- d %>% filter(success == 1) 
```

Tranform features.

```{r}
preProcValuesPred <- preProcess(select(d_features_preds, -frame, -timestamp, 
                                       -gaze_angle_x, -gaze_angle_y), 
                                method = c("center", "scale", "nzv"))
testTransformedPred <- predict(preProcValuesPred, d_features_preds)
```

Use GBM to generate predictions.

```{r}
testTransformedPred$pred <- predict(gbmFit, newdata = testTransformedPred)
```

Plot the timecourse to see if we get some reasonable looking runs of looking and not looking

200 frames ~ 1 minute of video.

Does it capture the timing of looks and no looks in the gold set?

Look 1

```{r}
testTransformedPred %>% 
  filter(frame >= 2992, frame <=3031) %>% 
  ggplot(aes(x = frame, y = pred, group = 1)) +
  geom_point() +
  geom_line()
```

Look 2

```{r}
testTransformedPred %>% 
  filter(frame >= 3087, frame <=3168) %>% 
  ggplot(aes(x = frame, y = pred, group = 1)) +
  geom_point() +
  geom_line()
```

Look 3

```{r}
testTransformedPred %>% 
  filter(frame >= 3189, frame <=3300) %>% 
  ggplot(aes(x = frame, y = pred, group = 1)) +
  geom_point() +
  geom_line()
```

My sense is that counting number of looks is going to be noisy using this classifier. We might need to think about some kind of interpolation step after generating predictions. Or we could take the previous look into account as feature in the classifier. The only way I can think of is to use a RNN. 

### Compute proportion looking for each trial/condition

The goal is to see if we can reproduce the results from Emily's paper based on number of looks with a proportion looking measure generated via the ML pipeline. 

```{r}
testTransformedPred %>% 
  filter(!is.na(phase), 
         str_detect(trial, pattern = "t")) %>% 
  mutate(trial = str_extract(trial, pattern = "[[:digit:]]+") %>% as.numeric()) %>% 
  ggplot(aes(x = frame, y = pred, group = 1, color = phase)) +
  geom_point() +
  geom_line() +
  facet_wrap(~trial, scales = "free_x", nrow = 4)
```

### Smooth the model predictions

[todo: smooth out these looking functions by interpolating between shifts that have <= 5 frames (<200 ms) for a fixation]

```{r}
test <- testTransformedPred %>% filter(trial == "t4")
```

```{r}
test_vec <- test$pred %>% as.character()
fix_length <- 5
# takes a vector of looks
# retuns a same length vector with short fixations removed (<5 frames)

for(idx in 1:length(test_vec)) {
  
  curr_look <- test_vec[idx]
  prev_look <- test_vec[idx - 1]
  
  if (curr_look != prev_look ) {
    fixation_check_window <- test_vec[idx:idx + fix_length] 
    
    if (!(curr_look %in% fixation_check_window)) {
      
    }
      
  }
}

```

```{r}
good_look <- c("look", "look", "look", "look", "look", "look")
bad_look <- c("look", "look", "no_look", "look", "look", "look")

!("look" %in% good_look)
```

## Test whether the classifer can generalize 

Can we use the model to predict looking for a new participant that the model has never seen before? 

[todo: get ground truth for a new participant and see how well the GBM model does.]

```{r}

```

