---
title: "Auto-Gaze build classifier"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T, warning = F, message = F, fig.width = 6, fig.asp = 0.63)
```

```{r}
library(cowplot); library(magrittr); 
library(tidyverse); library(caret); library(here)
theme_set(ggthemes::theme_few())
set.seed(96)
data_path <- "data/emily/ground_truth"
```

Read and merge separate gold code files using purrr map. 

```{r}
files <- list.files(here(data_path))
d <- files %>% map_dfr(~ read_csv(file.path(here(data_path, .))))
d_features <- read_csv(here("data/emily/results_openFace_03_3_18.csv"))
```

Join gold code with new features and select just the gaze relevant features.

```{r}
d %<>% 
  select(frame, timestamp, gold_code) %>% 
  left_join(., select(d_features, frame:gaze_angle_y, pose_Tx:pose_Rz))
```

## Preprocess the data

```{r}
d_model <- d %>% 
  filter(success == 1) %>% 
  select(-frame, -timestamp, -success, -pose_Tx, pose_Ty, pose_Tz) %>% 
  mutate(gold_code = as.factor(gold_code))
```

How many looks vs. no looks in the gold set?

```{r}
table(d_model$gold_code) 
```

## Visualize predictors

```{r}
d_model %>% 
  GGally::ggpairs(data = ., aes(color = gold_code, alpha = 0.6),
          columns = c("gaze_0_x", "gaze_0_y", "gaze_angle_x", "gaze_angle_y", 
                      "pose_Rx", "pose_Ry", "pose_Ty"))
```

Looks like gaze angle X and Y are perfectly correlated with gaze_X and gaze_Y. Generally, you want to remove attributes with an absolute correlation of 0.75 or higher.

```{r}
d_model %<>% select(-gaze_angle_x, -gaze_angle_y)
```

## Split the data

Into training and test sets.

```{r}
# identify the indices of 10 80% subsamples of the data
trainIndex <- createDataPartition(d_model$gold_code, p = .7, 
                                  list = FALSE, 
                                  times = 1)
head(trainIndex)
```

```{r}
train_data <- d_model[trainIndex, ]
test_data <- d_model[-trainIndex, ]
```

Preprocess (center and scale) the predictor variables.

```{r}
preProcValues <- preProcess(select(d_model, -gold_code), method = c("center", "scale", "nzv"))
trainTransformed <- predict(preProcValues, train_data)
testTransformed <- predict(preProcValues, test_data)
```

```{r}
table(trainTransformed$gold_code)
table(testTransformed$gold_code)
```

## Fit model

10-fold CV

```{r}
library(DMwR)
fitControl <- trainControl(method = "repeatedcv",
                           number = 10,
                           repeats = 10,
                           sampling = "smote",
                           savePredictions = T,
                           classProbs = T,
                           summaryFunction = twoClassSummary)
```

## Model tuning.

#### Fit decision tree with stochastic gradient boosting. 

The hyperparameters include:

* n.trees (# Boosting Iterations)
* interaction.depth (Max Tree Depth)
* shrinkage (Shrinkage)
* n.minobsinnode (Min. Terminal Node Size)

```{r}
gbmGrid <-  expand.grid(interaction.depth = c(5, 9, 14, 20), 
                        n.trees = c(200, 300, 400), 
                        shrinkage = 0.1,
                        n.minobsinnode = 20)

set.seed(825)
gbmFit <- train(gold_code ~ ., 
                 data = trainTransformed, 
                 method = "gbm", 
                 trControl = fitControl, 
                 verbose = FALSE, 
                 tuneGrid = gbmGrid)
gbmFit
```

KM notes on GBM hyperparameters:
  * fitting with interaction.depth of 1 doesn't do well, so I dropped this option
  * you start to see diminishing returns on Kappa and Accuracy after 300 trees and interaction.depth 20

#### Fit xgboost

Hyperparamters include: nrounds, max_depth, eta, gamma, colsample_bytree, min_child_weight, subsample

```{r, eval = F}
#todo
xgbGrid <-  expand.grid(interaction.depth = c(5, 9, 14, 20), 
                        n.trees = c(200, 300, 400), 
                        shrinkage = 0.1,
                        n.minobsinnode = 20)

xgbFit <- train(gold_code ~ ., 
                 data = trainTransformed, 
                 method = "xgbTree", 
                 metric = c("Accuracy", "Kappa"),
                 trControl = trainControl(method = "repeatedcv", number = 5, 
                                          repeats = 10, verboseIter = FALSE), 
                 verbose = FALSE)
```

## Evaluate model performance

### Plot accuracy and kappa

The Kappa Statistic compares the accuracy of the system to the accuracy of a random system. There is not a standardized interpretation of the kappa statistic. According to Wikipedia (citing their paper), Landis and Koch considers 0-0.20 as slight, 0.21-0.40 as fair, 0.41-0.60 as moderate, 0.61-0.80 as substantial, and 0.81-1 as almost perfect.

```{r}
ggplot(gbmFit, metric = "ROC") + labs(title="ROC")
```

### Plot ROC

```{r}
# todo
```

### Variable importance

```{r}
library(gbm)
gbmImp <- varImp(gbmFit, scale = FALSE)
plot(gbmImp)
```

Looks like pose_TZ and pose_TY are much more important than the rest of the predictors. Let's visualize the relationship between these predictors, gaze vectors, and gold codes.

```{r}
d_model %>% 
  GGally::ggpairs(data = ., aes(color = gold_code,  alpha = 0.6),
          columns = c("gaze_0_x", "gaze_0_y", "pose_Rx", "pose_Ty", "pose_Tz"))
```

### Accuracy

Get predictions from model on held out test set.

```{r get preds}
testTransformed$pred <- predict(gbmFit, newdata = testTransformed)
testTransformed %<>% mutate(correct_pred = ifelse(gold_code == pred, 1, 0))
```

Compute some stats to see how well did. First, accuracy overall:

```{r}
testTransformed %>% pull(correct_pred) %>% mean() %>% round(3)
```

Accuracy within each class:

```{r}
testTransformed %>% 
  group_by(gold_code) %>% 
  summarise(n = n(),
            acc = mean(correct_pred) %>% round(3))
```

### Precision and recall

Precision (also called positive predictive value) is the fraction of relevant instances among the retrieved instances, while recall (also known as sensitivity) is the fraction of relevant instances that have been retrieved over the total amount of relevant instances

```{r}
testTransformed %<>% 
  mutate(output_type = case_when(
    pred == "look" & gold_code == "look" ~ "true_positive",
    pred == "look" & gold_code == "no_look" ~ "false_positive",
    pred == "no_look" & gold_code == "no_look" ~ "true_negative",
    pred == "no_look" & gold_code == "look" ~ "false_negative")
    )

results <- testTransformed %>% count(output_type) 
```

Some functions to compute model performance.

```{r}
compute_precision <- function(true_positives, false_positives) {
  ( true_positives / (true_positives + false_positives) ) %>% round(2)
}

compute_recall <- function(true_positives, false_negatives) {
  ( true_positives / (true_positives + false_negatives) ) %>% round(2)
}

compute_f <- function(precision, recall) {
  ( (2 * precision * recall) / (precision + recall) ) %>% round(2)
}

```

```{r}
precision <- compute_precision(true_positives = results$n[4], false_positives = results$n[2])
paste0("precision is: ", precision)
```

```{r}
recall <- compute_recall(true_positives = results$n[4], false_negatives = results$n[1])
paste0("recall is: ", recall)
```

```{r}
f_score <- compute_f(precision, recall)
paste0("f-score is: ", f_score)
```

## Predict on full dataset

```{r}
d_features_preds <- d_features %>% 
  select(frame:gaze_angle_y, pose_Tx:pose_Rz) %>% 
  filter(success == 1) 
```

Tranform features.

```{r}
preProcValuesPred <- preProcess(select(d_features_preds, -frame, -timestamp, 
                                       -gaze_angle_x, -gaze_angle_y), 
                                method = c("center", "scale", "nzv"))
testTransformedPred <- predict(preProcValuesPred, d_features_preds)
```

Make predictions.

```{r}
testTransformedPred$pred <- predict(gbmFit, newdata = testTransformedPred)
```

Plot the timecourse to see if we get some reasonable looking runs of looking and not looking

200 frames ~ 1 minute of video.

Does it capture the timing of looks and no looks in the gold set?

Look 1

```{r}
testTransformedPred %>% 
  filter(frame >= 2992, frame <=3031) %>% 
  ggplot(aes(x = frame, y = pred, group = 1)) +
  geom_point() +
  geom_line()
```

Look 2

```{r}
testTransformedPred %>% 
  filter(frame >= 3087, frame <=3168) %>% 
  ggplot(aes(x = frame, y = pred, group = 1)) +
  geom_point() +
  geom_line()
```

Look 3

```{r}
testTransformedPred %>% 
  filter(frame >= 3189, frame <=3300) %>% 
  ggplot(aes(x = frame, y = pred, group = 1)) +
  geom_point() +
  geom_line()
```

My sense is that counting number of looks is going to be noisy using this classifier. We might need to think about some kind of interpolation step after generating predictions. Or we could take the previous look into account as feature in the classifier. The only way I can think of is to use a RNN. 

### Compute proportion looking for each trial/condition

The goal is to see if we can reproduce the results from Emily's paper based on number of looks with a proportion looking measure generated via the ML pipeline. 

todo: get trial timing and condition information from Emily

```{r}

```


## Test whether the classifer can generalize 

Can we used the model to predict looking for a new participant that it has never seen before? 

todo: get ground truth for a new participant and see how well the GBM model does. 

```{r}

```

