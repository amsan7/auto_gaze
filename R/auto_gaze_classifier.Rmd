---
title: "Auto-Gaze build classifier"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = F, message = F, fig.width = 8, fig.asp = 0.63)
```

```{r}
library(cowplot); library(here); library(magrittr); library(tidyverse)
library(caret)
theme_set(ggthemes::theme_few())
set.seed(96)
```

Read data

```{r, eval = F}
#d <- read_csv(here("data/kyle/results.csv"))
d <- read_csv(here("data/emily/gold_set_candidates.csv"))
d_features <- read_csv(here("data/emily/socref_21018_03_3_18 CV.csv"))
```

Join gold code with new features and select just the gaze relevant features.

```{r, eval = F}
d %<>% 
  select(frame, timestamp, gold_code) %>% 
  left_join(., select(d_features, frame:gaze_angle_y, pose_Tx:pose_Rz))
```

## Preprocess the data

```{r, eval = F}
d_model <- d %>% 
  filter(success == 1) %>% 
  select(-frame, -timestamp, -success, -pose_Tx, pose_Ty, pose_Tz) %>% 
  mutate(gold_code = as.factor(gold_code))
```

## Split the data

```{r}
# identify the indices of 10 80% subsamples of the data
trainIndex <- createDataPartition(d_model$gold_code, p = .3, 
                                  list = FALSE, 
                                  times = 1)
head(trainIndex)
```

```{r}
train_data <- d_model[trainIndex, ]
test_data <- d_model[-trainIndex, ]
```

Preprocess (center and scale) the predictor variables.

```{r}
preProcValues <- preProcess(select(d_model, -gold_code), method = c("center", "scale", "nzv"))
trainTransformed <- predict(preProcValues, train_data)
testTransformed <- predict(preProcValues, test_data)
```

## Fit model

10-fold CV

```{r}
library(DMwR)
fitControl <- trainControl(method = "repeatedcv",
                           number = 10,
                           repeats = 10,
                           sampling = "smote")
```

Fit decision tree with stochastic gradient boosting. The hyperparameters include:

* n.trees (# Boosting Iterations)
* interaction.depth (Max Tree Depth)
* shrinkage (Shrinkage)
* n.minobsinnode (Min. Terminal Node Size)

## Model tuning.

```{r}
gbmGrid <-  expand.grid(interaction.depth = c(1, 5, 9), 
                        n.trees = c(50, 100, 150, 200, 250, 300), 
                        shrinkage = 0.1,
                        n.minobsinnode = 20)

set.seed(825)
gbmFit2 <- train(gold_code ~ ., 
                 data = trainTransformed, 
                 method = "gbm", 
                 trControl = fitControl, 
                 verbose = FALSE, 
                 tuneGrid = gbmGrid)
gbmFit2
```

Plot accuracy and kappa value. The Kappa Statistic compares the accuracy of the system to the accuracy of a random system. There is not a standardized interpretation of the kappa statistic. According to Wikipedia (citing their paper), Landis and Koch considers 0-0.20 as slight, 0.21-0.40 as fair, 0.41-0.60 as moderate, 0.61-0.80 as substantial, and 0.81-1 as almost perfect.

```{r}
plot_grid(ggplot(gbmFit2, metric = "Kappa") + labs(title="kappa"), 
          ggplot(gbmFit2, metric = "Accuracy") + labs(title="accuracy"))
```

## Evaluate model

### Accuracy

```{r get preds}
testTransformed$pred <- predict(gbmFit2, newdata = testTransformed)
testTransformed %<>% mutate(correct_pred = ifelse(gold_code == pred, 1, 0))
```

How well did the model do 

```{r}
testTransformed %>% pull(correct_pred) %>% mean() %>% round(3)
```

```{r}
testTransformed %>% 
  group_by(gold_code) %>% 
  summarise(n = n(),
            acc = mean(correct_pred) %>% round(3))
```

### Precision and recall

precision (also called positive predictive value) is the fraction of relevant instances among the retrieved instances, while recall (also known as sensitivity) is the fraction of relevant instances that have been retrieved over the total amount of relevant instances

```{r}
testTransformed %<>% 
  mutate(output_type = case_when(
    pred == "look" & gold_code == "look" ~ "true_positive",
    pred == "look" & gold_code == "no_look" ~ "false_positive",
    pred == "no_look" & gold_code == "no_look" ~ "true_negative",
    pred == "no_look" & gold_code == "look" ~ "false_negative")
    )

results <- testTransformed %>% count(output_type) 
```

```{r}
paste0("precision is: ", round(results$n[4] / (results$n[2] + results$n[4]), 2))
paste0("recall is: ", round(results$n[4] / (results$n[1] + results$n[4]), 2))
```


